{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(12345)\n",
    "import tensorflow as tf\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(1234)\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import skimage\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, TensorBoard\n",
    "from keras import utils\n",
    "from tensorflow.keras import backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging\n",
    "\n",
    "First, we need to define some necessary logging utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainValTensorBoard(TensorBoard):\n",
    "    def __init__(self, log_dir='./log1', **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.writer = tf.summary.create_file_writer(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        with self.writer.as_default():\n",
    "            for name, value in val_logs.items():\n",
    "                tf.summary.scalar(name, value, step=epoch)\n",
    "            self.writer.flush()\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        logs.update({'lr': backend.eval(self.model.optimizer.lr)})\n",
    "        super(TrainValTensorBoard, self).on_epoch_end(epoch, logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "Now we need to tell it where to find the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "params = {'batch_size': 1,\n",
    "          'dim': (128,128,128),\n",
    "          'n_channels': 1,\n",
    "          'shuffle': True}\n",
    "seismPathT = 'data/train/seis/'\n",
    "faultPathT = 'data/train/fault/'\n",
    "\n",
    "seismPathV = 'data/validation/seis/'\n",
    "faultPathV = 'data/validation/fault/'\n",
    "train_ID = range(200)\n",
    "valid_ID = range(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization and Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(utils.Sequence):\n",
    "    'Generates data for keras'\n",
    "    def __init__(self, seismic_path, fault_path, data_IDs,\n",
    "                 batch_size=1, dim=(128,128,128),\n",
    "                 n_channels=1, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim   = dim\n",
    "        self.seismic_path = seismic_path\n",
    "        self.fault_path = fault_path\n",
    "        self.batch_size = batch_size\n",
    "        self.data_IDs = data_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.data_IDs)/self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        bsize = self.batch_size\n",
    "        indexes = self.indexes[index*bsize:(index+1)*bsize]\n",
    "\n",
    "        # Find list of IDs\n",
    "        data_IDs_temp = [self.data_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, Y = self.__data_generation(data_IDs_temp)\n",
    "\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.data_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, data_IDs_temp):\n",
    "        'Generates data containing batch_size samples'\n",
    "        # Initialization\n",
    "        \n",
    "        seismic = np.fromfile(f'{self.seismic_path}{data_IDs_temp[0]}.dat',\n",
    "                              dtype=np.single)\n",
    "        seismic = seismic.reshape((*self.dim, self.n_channels))\n",
    "        seismic -= seismic.mean()\n",
    "        seismic /= seismic.std()\n",
    "        seismic = seismic.transpose((2, 1, 0, 3))  # reorder\n",
    "        X = np.concatenate((seismic[np.newaxis, ...],\n",
    "                            seismic[np.newaxis, ::-1, ...]), axis=0)\n",
    "        del seismic\n",
    "        \n",
    "        fault = np.fromfile(f'{self.fault_path}{data_IDs_temp[0]}.dat',\n",
    "                            dtype=np.single)\n",
    "        fault = fault.reshape((*self.dim, self.n_channels)).transpose((2, 1, 0, 3))\n",
    "        Y = np.concatenate((fault[np.newaxis, ...],\n",
    "                            fault[np.newaxis, ::-1, ...]), axis=0)\n",
    "        del fault\n",
    "        return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DataGenerator at 0x7f6f7843c350>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator = DataGenerator(seismPathT, faultPathT,\n",
    "                                  data_IDs=train_ID,**params)\n",
    "valid_generator = DataGenerator(seismPathV, faultPathV,\n",
    "                                  data_IDs=valid_ID,**params)\n",
    "train_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Create the model (most of this code is in `unet3.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_45 (Conv3D)              (None, None, None, N 448         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_46 (Conv3D)              (None, None, None, N 6928        conv3d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_9 (MaxPooling3D)  (None, None, None, N 0           conv3d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_47 (Conv3D)              (None, None, None, N 13856       max_pooling3d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_48 (Conv3D)              (None, None, None, N 27680       conv3d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_10 (MaxPooling3D) (None, None, None, N 0           conv3d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_49 (Conv3D)              (None, None, None, N 55360       max_pooling3d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_50 (Conv3D)              (None, None, None, N 110656      conv3d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_11 (MaxPooling3D) (None, None, None, N 0           conv3d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_51 (Conv3D)              (None, None, None, N 221312      max_pooling3d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_52 (Conv3D)              (None, None, None, N 442496      conv3d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_9 (UpSampling3D)  (None, None, None, N 0           conv3d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, None, None, N 0           up_sampling3d_9[0][0]            \n",
      "                                                                 conv3d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_53 (Conv3D)              (None, None, None, N 331840      concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_54 (Conv3D)              (None, None, None, N 110656      conv3d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_10 (UpSampling3D) (None, None, None, N 0           conv3d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, None, None, N 0           up_sampling3d_10[0][0]           \n",
      "                                                                 conv3d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_55 (Conv3D)              (None, None, None, N 82976       concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_56 (Conv3D)              (None, None, None, N 27680       conv3d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_11 (UpSampling3D) (None, None, None, N 0           conv3d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, None, None, N 0           up_sampling3d_11[0][0]           \n",
      "                                                                 conv3d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_57 (Conv3D)              (None, None, None, N 20752       concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_58 (Conv3D)              (None, None, None, N 6928        conv3d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_59 (Conv3D)              (None, None, None, N 17          conv3d_58[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,459,585\n",
      "Trainable params: 1,459,585\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_45 (Conv3D)              (None, None, None, N 448         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_46 (Conv3D)              (None, None, None, N 6928        conv3d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_9 (MaxPooling3D)  (None, None, None, N 0           conv3d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_47 (Conv3D)              (None, None, None, N 13856       max_pooling3d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_48 (Conv3D)              (None, None, None, N 27680       conv3d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_10 (MaxPooling3D) (None, None, None, N 0           conv3d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_49 (Conv3D)              (None, None, None, N 55360       max_pooling3d_10[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_50 (Conv3D)              (None, None, None, N 110656      conv3d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling3d_11 (MaxPooling3D) (None, None, None, N 0           conv3d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_51 (Conv3D)              (None, None, None, N 221312      max_pooling3d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_52 (Conv3D)              (None, None, None, N 442496      conv3d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_9 (UpSampling3D)  (None, None, None, N 0           conv3d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, None, None, N 0           up_sampling3d_9[0][0]            \n",
      "                                                                 conv3d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_53 (Conv3D)              (None, None, None, N 331840      concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_54 (Conv3D)              (None, None, None, N 110656      conv3d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_10 (UpSampling3D) (None, None, None, N 0           conv3d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, None, None, N 0           up_sampling3d_10[0][0]           \n",
      "                                                                 conv3d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_55 (Conv3D)              (None, None, None, N 82976       concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_56 (Conv3D)              (None, None, None, N 27680       conv3d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling3d_11 (UpSampling3D) (None, None, None, N 0           conv3d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, None, None, N 0           up_sampling3d_11[0][0]           \n",
      "                                                                 conv3d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_57 (Conv3D)              (None, None, None, N 20752       concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_58 (Conv3D)              (None, None, None, N 6928        conv3d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3d_59 (Conv3D)              (None, None, None, N 17          conv3d_58[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,459,585\n",
      "Trainable params: 1,459,585\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def unet(pretrained_weights = None, input_size = (None,None,None,1)):\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv3D(16, (3,3,3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv3D(16, (3,3,3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling3D(pool_size=(2,2,2))(conv1)\n",
    "\n",
    "    conv2 = Conv3D(32, (3,3,3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv3D(32, (3,3,3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling3D(pool_size=(2,2,2))(conv2)\n",
    "\n",
    "    conv3 = Conv3D(64, (3,3,3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv3D(64, (3,3,3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling3D(pool_size=(2,2,2))(conv3)\n",
    "\n",
    "    conv4 = Conv3D(128, (3,3,3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv3D(128, (3,3,3), activation='relu', padding='same')(conv4)\n",
    "\n",
    "    up5 = concatenate([UpSampling3D(size=(2,2,2))(conv4), conv3], axis=-1)\n",
    "    conv5 = Conv3D(64, (3,3,3), activation='relu', padding='same')(up5)\n",
    "    conv5 = Conv3D(64, (3,3,3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "    up6 = concatenate([UpSampling3D(size=(2,2,2))(conv5), conv2], axis=-1)\n",
    "    conv6 = Conv3D(32, (3,3,3), activation='relu', padding='same')(up6)\n",
    "    conv6 = Conv3D(32, (3,3,3), activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = concatenate([UpSampling3D(size=(2,2,2))(conv6), conv1], axis=-1)\n",
    "    conv7 = Conv3D(16, (3,3,3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv3D(16, (3,3,3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "    conv8 = Conv3D(1, (1,1,1), activation='sigmoid')(conv7)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[conv8])\n",
    "    model.summary()\n",
    "    #model.compile(optimizer = Adam(lr = 1e-4), \n",
    "    #    loss = cross_entropy_balanced, metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "@tf.function\n",
    "def _to_tensor(x, dtype):\n",
    "    \"\"\"Convert the input `x` to a tensor of type `dtype`.\n",
    "    # Arguments\n",
    "    x: An object to be converted (numpy array, list, tensors).\n",
    "    dtype: The destination type.\n",
    "    # Returns\n",
    "    A tensor.\n",
    "    \"\"\"\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    if x.dtype != dtype:\n",
    "        x = tf.cast(x, dtype)\n",
    "    return x\n",
    "\n",
    "@tf.function\n",
    "def cross_entropy_balanced(y_true, y_pred):\n",
    "    # Note: tf.nn.sigmoid_cross_entropy_with_logits expects y_pred is logits, \n",
    "    # Keras expects probabilities.\n",
    "    # transform y_pred back to logits\n",
    "    _epsilon = _to_tensor(K.epsilon(), y_pred.dtype.base_dtype)\n",
    "    y_pred   = tf.clip_by_value(y_pred, _epsilon, 1 - _epsilon)\n",
    "    y_pred   = tf.log(y_pred/ (1 - y_pred))\n",
    "\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "    count_neg = tf.reduce_sum(1. - y_true)\n",
    "    count_pos = tf.reduce_sum(y_true)\n",
    "\n",
    "    beta = count_neg / (count_neg + count_pos)\n",
    "\n",
    "    pos_weight = beta / (1 - beta)\n",
    "\n",
    "    cost = tf.nn.weighted_cross_entropy_with_logits(logits=y_pred, targets=y_true, pos_weight=pos_weight)\n",
    "\n",
    "    cost = tf.reduce_mean(cost * (1 - beta))\n",
    "\n",
    "    return tf.where(tf.equal(count_pos, 0.0), 0.0, cost)\n",
    "\n",
    "\n",
    "\n",
    "model = unet(input_size=(None, None, None,1))\n",
    "model.compile(optimizer=Adam(lr=1e-4), loss='binary_crossentropy', \n",
    "                metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need to set up our logging (the TensorBoard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared. Ready to train!\n"
     ]
    }
   ],
   "source": [
    "# checkpoint\n",
    "filepath = 'check1/fseg-{epoch:02d}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', \n",
    "        verbose=1, save_best_only=False, mode='max')\n",
    "logging = TrainValTensorBoard()\n",
    "#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, \n",
    "#                              patience=20, min_lr=1e-8)\n",
    "callbacks_list = [checkpoint, logging]\n",
    "print('Data prepared. Ready to train!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model\n",
    "\n",
    "Finally we are ready to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 17/200 [=>............................] - ETA: 55:45 - loss: 0.5399 - accuracy: 0.7956"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit(x=train_generator,\n",
    "                    validation_data=valid_generator,\n",
    "                    epochs=100,\n",
    "                    callbacks=callbacks_list,\n",
    "                    verbose=1)\n",
    "model.save('check1/fseg.hdf5')\n",
    "print('Model saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Results\n",
    "\n",
    "Let's see what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(history.history['acc'])\n",
    "ax.plot(history.history['val_acc'])\n",
    "ax.title('Model accuracy', fontsize=20)\n",
    "ax.xlabel('Epoch', fontsize=20)\n",
    "ax.ylabel('Accuracy', fontsize=20)\n",
    "ax.legend(['train', 'test'], loc='center right', fontsize=20)\n",
    "ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=18)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(history.history['loss'])\n",
    "ax.plot(history.history['val_loss'])\n",
    "ax.title('Model loss',fontsize=20)\n",
    "ax.ylabel('Loss',fontsize=20)\n",
    "ax.xlabel('Epoch',fontsize=20)\n",
    "ax.legend(['train', 'test'], loc='center right',fontsize=20)\n",
    "ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=18)\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
