{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(12345)\n",
    "import tensorflow as tf\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(1234)\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import skimage\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau, TensorBoard\n",
    "from keras import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging\n",
    "\n",
    "First, we need to define some necessary logging utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainValTensorBoard(TensorBoard):\n",
    "    def __init__(self, log_dir='./log1', **kwargs):\n",
    "        # Make the original `TensorBoard` log to a subdirectory 'training'\n",
    "        training_log_dir = os.path.join(log_dir, 'training')\n",
    "        super(TrainValTensorBoard, self).__init__(training_log_dir, **kwargs)\n",
    "        # Log the validation metrics to a separate subdirectory\n",
    "        self.val_log_dir = os.path.join(log_dir, 'validation')\n",
    "\n",
    "    def set_model(self, model):\n",
    "        # Setup writer for validation metrics\n",
    "        self.writer = tf.summary.create_file_writer(self.val_log_dir)\n",
    "        super(TrainValTensorBoard, self).set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Pop the validation logs and handle them separately with\n",
    "        # `self.writer`. Also rename the keys so that they can\n",
    "        # be plotted on the same figure with the training metrics\n",
    "        logs = logs or {}\n",
    "        val_logs = {k.replace('val_', ''): v for k, v in logs.items() if k.startswith('val_')}\n",
    "        with self.writer.as_default():\n",
    "            for name, value in val_logs.items():\n",
    "                tf.summary.scalar(name, value, step=epoch)\n",
    "            self.writer.flush()\n",
    "        # Pass the remaining logs to `TensorBoard.on_epoch_end`\n",
    "        logs = {k: v for k, v in logs.items() if not k.startswith('val_')}\n",
    "        logs.update({'lr': keras.eval(self.model.optimizer.lr)})\n",
    "        super(TrainValTensorBoard, self).on_epoch_end(epoch, logs)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        super(TrainValTensorBoard, self).on_train_end(logs)\n",
    "        self.val_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "Now we need to tell it where to find the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "params = {'batch_size': 1,\n",
    "          'dim': (128,128,128),\n",
    "          'n_channels': 1,\n",
    "          'shuffle': True}\n",
    "seismPathT = './data/train/seis/'\n",
    "faultPathT = './data/train/fault/'\n",
    "\n",
    "seismPathV = './data/validation/seis/'\n",
    "faultPathV = './data/validation/fault/'\n",
    "train_ID = range(200)\n",
    "valid_ID = range(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization and Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(utils.Sequence):\n",
    "    'Generates data for keras'\n",
    "    def __init__(self,dpath,fpath,data_IDs, batch_size=1, dim=(128,128,128), \n",
    "                 n_channels=1, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim   = dim\n",
    "        self.dpath = dpath\n",
    "        self.fpath = fpath\n",
    "        self.batch_size = batch_size\n",
    "        self.data_IDs   = data_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.shuffle    = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.data_IDs)/self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        bsize = self.batch_size\n",
    "        indexes = self.indexes[index*bsize:(index+1)*bsize]\n",
    "\n",
    "        # Find list of IDs\n",
    "        data_IDs_temp = [self.data_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, Y = self.__data_generation(data_IDs_temp)\n",
    "\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.data_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, data_IDs_temp):\n",
    "        'Generates data containing batch_size samples'\n",
    "        # Initialization\n",
    "        gx  = np.fromfile(self.dpath+str(data_IDs_temp[0])+'.dat',dtype=np.single)\n",
    "        fx = np.fromfile(self.fpath+str(data_IDs_temp[0])+'.dat',dtype=np.single)\n",
    "        gx = np.reshape(gx,self.dim)\n",
    "        fx = np.reshape(fx,self.dim)\n",
    "        xm = np.mean(gx)\n",
    "        xs = np.std(gx)\n",
    "        gx = gx-xm\n",
    "        gx = gx/xs\n",
    "        gx = np.transpose(gx)\n",
    "        fx = np.transpose(fx)\n",
    "        #in seismic processing, the dimensions of a seismic array is often arranged as\n",
    "        #a[n3][n2][n1] where n1 represnts the vertical dimenstion. This is why we need \n",
    "        #to transpose the array here in python \n",
    "        # Generate data\n",
    "        X = np.zeros((2, *self.dim, self.n_channels),dtype=np.single)\n",
    "        Y = np.zeros((2, *self.dim, self.n_channels),dtype=np.single)\n",
    "        X[0,] = np.reshape(gx, (*self.dim,self.n_channels))\n",
    "        Y[0,] = np.reshape(fx, (*self.dim,self.n_channels))\n",
    "        X[1,] = np.reshape(np.flipud(gx), (*self.dim,self.n_channels))\n",
    "        Y[1,] = np.reshape(np.flipud(fx), (*self.dim,self.n_channels))\n",
    "        return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = DataGenerator(dpath=seismPathT,fpath=faultPathT,\n",
    "                                  data_IDs=train_ID,**params)\n",
    "valid_generator = DataGenerator(dpath=seismPathV,fpath=faultPathV,\n",
    "                                  data_IDs=valid_ID,**params)\n",
    "train_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Create the model (most of this code is in `unet3.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet(pretrained_weights = None,input_size = (None,None,None,1)):\n",
    "    inputs = Input(input_size)\n",
    "    conv1 = Conv3D(16, (3,3,3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv3D(16, (3,3,3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling3D(pool_size=(2,2,2))(conv1)\n",
    "\n",
    "    conv2 = Conv3D(32, (3,3,3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv3D(32, (3,3,3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling3D(pool_size=(2,2,2))(conv2)\n",
    "\n",
    "    conv3 = Conv3D(64, (3,3,3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv3D(64, (3,3,3), activation='relu', padding='same')(conv3)\n",
    "    pool3 = MaxPooling3D(pool_size=(2,2,2))(conv3)\n",
    "\n",
    "    conv4 = Conv3D(128, (3,3,3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = Conv3D(128, (3,3,3), activation='relu', padding='same')(conv4)\n",
    "\n",
    "    up5 = concatenate([UpSampling3D(size=(2,2,2))(conv4), conv3], axis=-1)\n",
    "    conv5 = Conv3D(64, (3,3,3), activation='relu', padding='same')(up5)\n",
    "    conv5 = Conv3D(64, (3,3,3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "    up6 = concatenate([UpSampling3D(size=(2,2,2))(conv5), conv2], axis=-1)\n",
    "    conv6 = Conv3D(32, (3,3,3), activation='relu', padding='same')(up6)\n",
    "    conv6 = Conv3D(32, (3,3,3), activation='relu', padding='same')(conv6)\n",
    "\n",
    "    up7 = concatenate([UpSampling3D(size=(2,2,2))(conv6), conv1], axis=-1)\n",
    "    conv7 = Conv3D(16, (3,3,3), activation='relu', padding='same')(up7)\n",
    "    conv7 = Conv3D(16, (3,3,3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "    conv8 = Conv3D(1, (1,1,1), activation='sigmoid')(conv7)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[conv8])\n",
    "    model.summary()\n",
    "    #model.compile(optimizer = Adam(lr = 1e-4), \n",
    "    #    loss = cross_entropy_balanced, metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "def cross_entropy_balanced(y_true, y_pred):\n",
    "    # Note: tf.nn.sigmoid_cross_entropy_with_logits expects y_pred is logits, \n",
    "    # Keras expects probabilities.\n",
    "    # transform y_pred back to logits\n",
    "    _epsilon = _to_tensor(K.epsilon(), y_pred.dtype.base_dtype)\n",
    "    y_pred   = tf.clip_by_value(y_pred, _epsilon, 1 - _epsilon)\n",
    "    y_pred   = tf.log(y_pred/ (1 - y_pred))\n",
    "\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "\n",
    "    count_neg = tf.reduce_sum(1. - y_true)\n",
    "    count_pos = tf.reduce_sum(y_true)\n",
    "\n",
    "    beta = count_neg / (count_neg + count_pos)\n",
    "\n",
    "    pos_weight = beta / (1 - beta)\n",
    "\n",
    "    cost = tf.nn.weighted_cross_entropy_with_logits(logits=y_pred, targets=y_true, pos_weight=pos_weight)\n",
    "\n",
    "    cost = tf.reduce_mean(cost * (1 - beta))\n",
    "\n",
    "    return tf.where(tf.equal(count_pos, 0.0), 0.0, cost)\n",
    "\n",
    "\n",
    "def _to_tensor(x, dtype):\n",
    "    \"\"\"Convert the input `x` to a tensor of type `dtype`.\n",
    "    # Arguments\n",
    "    x: An object to be converted (numpy array, list, tensors).\n",
    "    dtype: The destination type.\n",
    "    # Returns\n",
    "    A tensor.\n",
    "    \"\"\"\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    if x.dtype != dtype:\n",
    "        x = tf.cast(x, dtype)\n",
    "    return x\n",
    "\n",
    "model = unet(input_size=(None, None, None,1))\n",
    "model.compile(optimizer=Adam(lr=1e-4), loss='binary_crossentropy', \n",
    "                metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need to set up our logging (the TensorBoard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "filepath = 'check1/fseg-{epoch:02d}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', \n",
    "        verbose=1, save_best_only=False, mode='max')\n",
    "logging = TrainValTensorBoard()\n",
    "#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, \n",
    "#                              patience=20, min_lr=1e-8)\n",
    "callbacks_list = [checkpoint, logging]\n",
    "print('Data prepared. Ready to train!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model\n",
    "\n",
    "Finally we are ready to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(3, (4,4), input_shape=(100,100,4)))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "history = model.fit(x=train_generator,\n",
    "                    validation_data=valid_generator,\n",
    "                    epochs=100,\n",
    "                    callbacks=callbacks_list,\n",
    "                    verbose=1)\n",
    "model.save('check1/fseg.hdf5')\n",
    "print('Model saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Results\n",
    "\n",
    "Let's see what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all data in history\n",
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(history.history['acc'])\n",
    "ax.plot(history.history['val_acc'])\n",
    "ax.title('Model accuracy', fontsize=20)\n",
    "ax.xlabel('Epoch', fontsize=20)\n",
    "ax.ylabel('Accuracy', fontsize=20)\n",
    "ax.legend(['train', 'test'], loc='center right', fontsize=20)\n",
    "ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=18)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(history.history['loss'])\n",
    "ax.plot(history.history['val_loss'])\n",
    "ax.title('Model loss',fontsize=20)\n",
    "ax.ylabel('Loss',fontsize=20)\n",
    "ax.xlabel('Epoch',fontsize=20)\n",
    "ax.legend(['train', 'test'], loc='center right',fontsize=20)\n",
    "ax.tick_params(axis='both', which='major', labelsize=18)\n",
    "ax.tick_params(axis='both', which='minor', labelsize=18)\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
